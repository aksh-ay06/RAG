{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "Build a production-grade RAG system using Docker, PostgreSQL, OpenSearch, FastAPI, Airflow, and Ollama.\n",
    "\n",
    "## Technology Stack\n",
    "| Component | Purpose | Port |\n",
    "|-----------|---------|------|\n",
    "| **FastAPI** | REST API | 8000 |\n",
    "| **PostgreSQL** | Paper metadata storage | 5432 |\n",
    "| **OpenSearch** | Hybrid search engine | 9200/5601 |\n",
    "| **Apache Airflow** | Workflow automation | 8080 |\n",
    "| **Ollama** | Local LLM inference | 11434 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74qckgs5icl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.3\n",
      "Environment: /media/akshay/DATA/Portfolio/RAG/.venv/bin/python3\n",
      "✓ Python version compatible\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "python_version = sys.version_info\n",
    "print(f\"Python Version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "if python_version >= (3, 12):\n",
    "    print(\"✓ Python version compatible\")\n",
    "else:\n",
    "    print(\"✗ Need Python 3.12+\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12izzsax7tmq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: /media/akshay/DATA/Portfolio/RAG\n"
     ]
    }
   ],
   "source": [
    "# Find Project Root\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "if current_dir.name == \"module1\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"✓ Project root: {project_root}\")\n",
    "else:\n",
    "    print(\"✗ Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "step2-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Docker: Docker version 28.3.3, build 980b856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Docker\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"docker\", \"--version\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ Docker: {result.stdout}\")\n",
    "    else:\n",
    "        print(\"✗ Docker: Not working\")\n",
    "        exit()\n",
    "except:\n",
    "    print(\"✗ Docker: Not found\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cue2b9j33ho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Docker Compose: v2.39.1\n"
     ]
    }
   ],
   "source": [
    "# Check Docker Compose\n",
    "try:\n",
    "    result = subprocess.run([\"docker\", \"compose\", \"version\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ Docker Compose: {result.stdout.split()[3]}\")\n",
    "    else:\n",
    "        print(\"✗ Docker Compose: Not working\")\n",
    "        exit()\n",
    "except:\n",
    "    print(\"✗ Docker Compose: Not found\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "k6oz19mcke8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ UV: uv 0.7.21\n",
      "\n",
      "✓ All required software ready!\n"
     ]
    }
   ],
   "source": [
    "# Check UV Package Manager\n",
    "try:\n",
    "    result = subprocess.run([\"uv\", \"--version\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ UV: {result.stdout.strip()}\")\n",
    "        print(\"\\n✓ All required software ready!\")\n",
    "    else:\n",
    "        print(\"✗ UV: Not working\")\n",
    "        exit()\n",
    "except:\n",
    "    print(\"✗ UV: Not found\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "l4vhkj6bl7h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Docker is running\n"
     ]
    }
   ],
   "source": [
    "# Check Docker Running\n",
    "try:\n",
    "    result = subprocess.run([\"docker\", \"info\"], capture_output=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Docker is running\")\n",
    "    else:\n",
    "        print(\"✗ Docker not running - start Docker Desktop\")\n",
    "        exit()\n",
    "except:\n",
    "    print(\"✗ Docker daemon not accessible\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1yuulcv2wqe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current containers:\n",
      "  • airflow: running\n",
      "  • api: running\n",
      "  • opensearch-dashboards: running\n",
      "  • ollama: running\n",
      "  • opensearch: running\n",
      "  • postgres: running\n"
     ]
    }
   ],
   "source": [
    "# Check Current Containers\n",
    "import json\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        print(\"Current containers:\")\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    container = json.loads(line)\n",
    "                    service = container.get('Service', 'unknown')\n",
    "                    state = container.get('State', 'unknown')\n",
    "                    print(f\"  • {service}: {state}\")\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        print(\"No containers running\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Could not check containers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ql8vfnm1iq",
   "metadata": {},
   "source": [
    "## Service Health Verification\n",
    "\n",
    "All services start automatically. Check their health status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77j1d8uyv9j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE STATUS\n",
      "======================================================================\n",
      "Service              State           Status          Notes\n",
      "----------------------------------------------------------------------\n",
      "✓ airflow            running        healthy        Ready\n",
      "✓ api                running        healthy        Ready\n",
      "✓ opensearch-dashboards running        healthy        Ready\n",
      "✓ ollama             running        healthy        Ready\n",
      "✓ opensearch         running        healthy        Ready\n",
      "✓ postgres           running        healthy        Ready\n"
     ]
    }
   ],
   "source": [
    "# Service Health Check\n",
    "EXPECTED_SERVICES = {\n",
    "    'api': 'FastAPI REST API server',\n",
    "    'postgres': 'PostgreSQL database',\n",
    "    'opensearch': 'OpenSearch search engine', \n",
    "    'opensearch-dashboards': 'OpenSearch web dashboard',\n",
    "    'ollama': 'Local LLM inference server',\n",
    "    'airflow': 'Workflow automation (optional - may be off)'\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=15\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"SERVICE STATUS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Service':<20} {'State':<15} {'Status':<15} {'Notes'}\")\n",
    "        print(\"-\" * 70)\n",
    "    else:\n",
    "        print(\"Could not get service status\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking services: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Parse Service Status\n",
    "found_services = set()\n",
    "service_states = {}\n",
    "\n",
    "if result.stdout.strip():\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        if line.strip():\n",
    "            try:\n",
    "                container = json.loads(line)\n",
    "                service = container.get('Service', 'unknown')\n",
    "                state = container.get('State', 'unknown')\n",
    "                health = container.get('Health', 'no check')\n",
    "                \n",
    "                found_services.add(service)\n",
    "                service_states[service] = {'state': state, 'health': health}\n",
    "                \n",
    "                if state == 'running' and health in ['healthy', 'no check']:\n",
    "                    indicator = \"✓\"\n",
    "                    notes = \"Ready\"\n",
    "                elif state == 'running' and health == 'unhealthy':\n",
    "                    indicator = \"⚠\"\n",
    "                    notes = \"Starting up...\"\n",
    "                elif state == 'exited':\n",
    "                    indicator = \"✗\"\n",
    "                    notes = \"Failed to start\"\n",
    "                else:\n",
    "                    indicator = \"?\"\n",
    "                    notes = f\"Status: {state}\"\n",
    "                \n",
    "                print(f\"{indicator} {service:<18} {state:<14} {health:<14} {notes}\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393qfrwg7h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Missing Services\n",
    "missing_services = set(EXPECTED_SERVICES.keys()) - found_services\n",
    "\n",
    "if missing_services:\n",
    "    print(\"\\nMISSING SERVICES:\")\n",
    "    print(\"-\" * 70)\n",
    "    for service in missing_services:\n",
    "        description = EXPECTED_SERVICES[service]\n",
    "        if service == 'airflow':\n",
    "            print(f\"⚠ {service:<18} not running    {'(Optional)':<14} {description}\")\n",
    "        else:\n",
    "            print(f\"✗ {service:<18} not running    {'Required':<14} {description}\")\n",
    "\n",
    "failed_services = [s for s, info in service_states.items() \n",
    "                  if info['state'] in ['exited', 'restarting'] or info['health'] == 'unhealthy']\n",
    "\n",
    "if failed_services:\n",
    "    print(f\"\\nTROUBLESHOOTING:\")\n",
    "    for service in failed_services:\n",
    "        print(f\"   docker compose logs {service}\")\n",
    "elif missing_services and 'airflow' not in missing_services:\n",
    "    print(f\"\\nACTION NEEDED:\")\n",
    "    print(\"Start missing services: docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rhnz43uolf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FastAPI is responding\n",
      "Status: ok\n"
     ]
    }
   ],
   "source": [
    "# Test FastAPI Health\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"✓ FastAPI is responding\")\n",
    "        print(f\"Status: {data.get('status', 'unknown')}\")\n",
    "    else:\n",
    "        print(f\"⚠ API returned status: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"✗ API not responding - wait 1-2 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ API test error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7uu7h40rutn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airflow Access:\n",
      "========================================\n",
      "URL: http://localhost:8080\n",
      "Username: admin\n",
      "Password: admin\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Airflow Connection Info\n",
    "print(\"Airflow Access:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"URL: http://localhost:8080\")\n",
    "print(\"Username: admin\")\n",
    "print(\"Password: admin\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950853cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Airflow is healthy\n",
      "\n",
      "Metadatabase: healthy\n",
      "Scheduler: healthy\n",
      "\n",
      "Airflow Login:\n",
      "URL: http://localhost:8080\n",
      "Username: admin\n",
      "Password: admin\n"
     ]
    }
   ],
   "source": [
    "# Test Airflow Health\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8080/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        health_data = response.json()\n",
    "        print(\"✓ Airflow is healthy\")\n",
    "        \n",
    "        # Show status details\n",
    "        metadatabase = health_data.get('metadatabase', {})\n",
    "        scheduler = health_data.get('scheduler', {})\n",
    "        \n",
    "        print(f\"\\nMetadatabase: {metadatabase.get('status', 'unknown')}\")\n",
    "        print(f\"Scheduler: {scheduler.get('status', 'unknown')}\")\n",
    "        \n",
    "        print(f\"\\nAirflow Login:\")\n",
    "        print(f\"URL: http://localhost:8080\")\n",
    "        print(f\"Username: admin\")\n",
    "        print(f\"Password: admin\")\n",
    "    else:\n",
    "        print(f\"⚠ Airflow returned: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"✗ Airflow not responding - wait 2-3 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Airflow test error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lie8ph4ilsb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenSearch Dashboards is accessible!\n",
      "✓ Web interface is ready for exploration\n",
      "\n",
      " Web Interface Access:\n",
      "========================================\n",
      "Main Dashboard: http://localhost:5601\n",
      "Dev Tools: http://localhost:5601/app/dev_tools\n",
      "========================================\n",
      "\n",
      " Student Learning Activities:\n",
      "1. Explore the Dashboard:\n",
      "   • Visit http://localhost:5601\n",
      "   • Navigate through the interface\n",
      "   • Check out the 'Discover' tab\n",
      "\n",
      "2. Use Dev Tools for API Queries:\n",
      "   • Go to Dev Tools\n",
      "   • Try: GET /_cluster/health\n",
      "   • Try: GET /_cat/indices?v\n",
      "   • Try: GET /_cluster/stats\n",
      "   • Check the learning material for more information\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check OpenSearch Dashboards Web Interface\n",
    "# This is the proper way for students to interact with OpenSearch\n",
    "\n",
    "dashboards_url = \"http://localhost:5601\"\n",
    "\n",
    "try:\n",
    "    # Test if Dashboards is accessible\n",
    "    response = requests.get(f\"{dashboards_url}/api/status\", timeout=10, allow_redirects=True)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✓ OpenSearch Dashboards is accessible!\")\n",
    "        print(\"✓ Web interface is ready for exploration\")\n",
    "        \n",
    "        print(\"\\n Web Interface Access:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Main Dashboard: {dashboards_url}\")\n",
    "        print(f\"Dev Tools: {dashboards_url}/app/dev_tools\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(\"\\n Student Learning Activities:\")\n",
    "        print(\"1. Explore the Dashboard:\")\n",
    "        print(\"   • Visit http://localhost:5601\")\n",
    "        print(\"   • Navigate through the interface\")\n",
    "        print(\"   • Check out the 'Discover' tab\")\n",
    "        \n",
    "        print(\"\\n2. Use Dev Tools for API Queries:\")\n",
    "        print(\"   • Go to Dev Tools\")\n",
    "        print(\"   • Try: GET /_cluster/health\")\n",
    "        print(\"   • Try: GET /_cat/indices?v\")\n",
    "        print(\"   • Try: GET /_cluster/stats\")\n",
    "        print(\"   • Check the learning material for more information\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠ Dashboards returned status: {response.status_code}\")\n",
    "        print(\"Interface may still be starting up\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"✗ OpenSearch Dashboards not accessible yet\")\n",
    "    print(\"Wait 2-3 minutes for full startup\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"⚠ Dashboards request timed out\")\n",
    "    print(\"This is normal during startup - try again in a few minutes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error accessing Dashboards: {e}\")\n",
    "    print(\"Check container status: docker compose ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "olb9gxpkomk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama is running!\n",
      "Available models: 0\n",
      "\n",
      "  No models installed yet\n",
      "   This is normal - models are large files (3-7 GB each)\n",
      "   In Week 4, we'll install a model like llama3.2\n",
      "\n",
      "  Try This Later (Week 4):\n",
      "1. docker exec -it rag-ollama ollama pull llama3.2\n",
      "2. docker exec -it rag-ollama ollama list\n",
      "3. docker exec -it rag-ollama ollama run llama3.2\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check Ollama Service Status\n",
    "# Let's see if Ollama is running and what models are available\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "ollama_url = \"http://localhost:11434/api/tags\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(ollama_url, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models_data = response.json()\n",
    "        models = models_data.get('models', [])\n",
    "        \n",
    "        print(\"✓ Ollama is running!\")\n",
    "        print(f\"Available models: {len(models)}\")\n",
    "        \n",
    "        if models:\n",
    "            print(\"\\nInstalled Models:\")\n",
    "            for model in models:\n",
    "                name = model.get('name', 'unknown')\n",
    "                size = model.get('size', 0)\n",
    "                size_gb = round(size / (1024**3), 1)\n",
    "                print(f\"  • {name} ({size_gb} GB)\")\n",
    "        else:\n",
    "            print(\"\\n  No models installed yet\")\n",
    "            print(\"   This is normal - models are large files (3-7 GB each)\")\n",
    "            print(\"   In Week 4, we'll install a model like llama3.2\")\n",
    "            \n",
    "        print(\"\\n  Try This Later (Week 4):\")\n",
    "        print(\"1. docker exec -it rag-ollama ollama pull llama3.2\")\n",
    "        print(\"2. docker exec -it rag-ollama ollama list\")\n",
    "        print(\"3. docker exec -it rag-ollama ollama run llama3.2\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠ Ollama returned status: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"✗ Ollama is not responding yet\")\n",
    "    print(\"Ollama service might still be starting\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"✗ Ollama request timed out\")\n",
    "    print(\"Service might still be initializing\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Unexpected error testing Ollama: {e}\")\n",
    "    print(\"Try again in a few minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rgr25nmpdx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama API is healthy!\n",
      "Version: 0.11.2\n",
      "\n",
      "  What is Ollama?\n",
      "• Runs AI models completely on your local machine\n",
      "• No data sent to external services (privacy-first)\n",
      "• No API fees or rate limits\n",
      "• Supports models like Llama, Mistral, Phi, etc.\n",
      "\n",
      "  Coming in Week 4:\n",
      "• Install and run a local language model\n",
      "• Generate answers to research questions\n",
      "• Summarize academic papers\n",
      "• All processing stays on your computer!\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Check Ollama Version and Health\n",
    "# Let's verify Ollama is properly configured\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "ollama_version_url = \"http://localhost:11434/api/version\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(ollama_version_url, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        version_data = response.json()\n",
    "        version = version_data.get('version', 'unknown')\n",
    "        \n",
    "        print(\"✓ Ollama API is healthy!\")\n",
    "        print(f\"Version: {version}\")\n",
    "        \n",
    "        print(\"\\n  What is Ollama?\")\n",
    "        print(\"• Runs AI models completely on your local machine\")\n",
    "        print(\"• No data sent to external services (privacy-first)\")\n",
    "        print(\"• No API fees or rate limits\")\n",
    "        print(\"• Supports models like Llama, Mistral, Phi, etc.\")\n",
    "        \n",
    "        print(\"\\n  Coming in Week 4:\")\n",
    "        print(\"• Install and run a local language model\")\n",
    "        print(\"• Generate answers to research questions\")\n",
    "        print(\"• Summarize academic papers\")\n",
    "        print(\"• All processing stays on your computer!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠ Ollama version check returned: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"✗ Could not check Ollama version\")\n",
    "    print(\"Service might still be starting up\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"✗ Ollama request timed out\")\n",
    "    print(\"Service might still be initializing\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Unexpected error checking version: {e}\")\n",
    "    print(\"Try again in a few minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1w3j46f69ge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADING LLAMA 3.2:1B MODEL\n",
      "==================================================\n",
      "This is a small 1.3GB model - perfect for testing!\n",
      "Download will take 2-5 minutes depending on your internet speed...\n",
      "Llama 3.2:1b model downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# HANDS-ON: Pull and Test Llama 3.2 (Small Model)\n",
    "\n",
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"DOWNLOADING LLAMA 3.2:1B MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This is a small 1.3GB model - perfect for testing!\")\n",
    "print(\"Download will take 2-5 minutes depending on your internet speed...\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"exec\", \"rag-ollama\", \"ollama\", \"pull\", \"llama3.2:1b\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=600\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Llama 3.2:1b model downloaded successfully!\")\n",
    "    else:\n",
    "        print(f\"Download issue: {result.stderr}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"Download timed out - this is normal for slow connections\")\n",
    "    print(\"The download continues in the background\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading model: {e}\")\n",
    "    print(\"Make sure Ollama container is running: docker compose ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4qa8r07k01v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing llama3.2:1b with prompt: 'What is machine learning in one sentence?'\n",
      "------------------------------------------------------------\n",
      "Generating response (this may take 10-30 seconds)...\n",
      "Response generated in 6.5 seconds\n",
      "\n",
      "RESPONSE:\n",
      "========================================\n",
      "Machine learning is a subfield of artificial intelligence that enables computers to learn and improve from data without being explicitly programmed, by identifying patterns and making predictions or decisions based on experience.\n",
      "========================================\n",
      "\n",
      "Model: llama3.2:1b\n",
      "Generation time: 6495ms\n",
      "\n",
      "SUCCESS! Your local AI model is working!\n",
      "\n",
      "Try more prompts:\n",
      "• test_ollama_model(\"llama3.2:1b\", \"Explain neural networks simply\")\n",
      "• test_ollama_model(\"llama3.2:1b\", \"Write a Python function to sort a list\")\n"
     ]
    }
   ],
   "source": [
    "# Test Llama 3.2:1b API\n",
    "\n",
    "def test_ollama_model(model_name, prompt, max_wait_time=60):\n",
    "    \"\"\"Test an Ollama model with a prompt.\"\"\"\n",
    "    print(f\"Testing {model_name} with prompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Generating response (this may take 10-30 seconds)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = requests.post(url, json=data, timeout=max_wait_time)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            response_text = result.get('response', '').strip()\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Response generated in {elapsed_time:.1f} seconds\")\n",
    "            print(\"\\nRESPONSE:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(response_text)\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            if 'model' in result:\n",
    "                print(f\"\\nModel: {result['model']}\")\n",
    "            if 'total_duration' in result:\n",
    "                duration_ms = result['total_duration'] / 1000000\n",
    "                print(f\"Generation time: {duration_ms:.0f}ms\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"API error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Could not connect to Ollama API\")\n",
    "        print(\"Make sure Ollama is running: docker compose ps\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Request timed out\")\n",
    "        print(\"Model might be loading for the first time (this is normal)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "test_prompt = \"What is machine learning in one sentence?\"\n",
    "success = test_ollama_model(\"llama3.2:1b\", test_prompt)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nSUCCESS! Your local AI model is working!\")\n",
    "    print(\"\\nTry more prompts:\")\n",
    "    print('• test_ollama_model(\"llama3.2:1b\", \"Explain neural networks simply\")')\n",
    "    print('• test_ollama_model(\"llama3.2:1b\", \"Write a Python function to sort a list\")')\n",
    "else:\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure model downloaded: docker exec rag-ollama ollama list\")\n",
    "    print(\"2. Check Ollama logs: docker compose logs ollama\")\n",
    "    print(\"3. Try again - first run takes longer to load model into memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bh1ic7xxxrq",
   "metadata": {},
   "source": [
    "### 5. PostgreSQL - Database Storage\n",
    "\n",
    "**Interactive Exploration:**\n",
    "\n",
    "PostgreSQL stores all structured data for our application:\n",
    "- **Connection**: localhost:5432\n",
    "- **Database**: rag_db\n",
    "- **Username/Password**: rag_user / rag_password\n",
    "- **GUI Tool Recommendation**: DBeaver (free database client)\n",
    "\n",
    "Let's test the database connection and explore the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "qjyjq3s023m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PostgreSQL is accepting connections on port 5432!\n",
      "\n",
      "  Database Connection Details:\n",
      "• Host: localhost\n",
      "• Port: 5432\n",
      "• Database: rag_db\n",
      "• Username: rag_user\n",
      "• Password: rag_password\n",
      "\n",
      "  Recommended GUI Tools:\n",
      "• DBeaver (Free): https://dbeaver.io/download/\n",
      "• pgAdmin: https://www.pgadmin.org/download/\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check PostgreSQL Connection (Basic)\n",
    "# Let's verify PostgreSQL is accepting connections\n",
    "\n",
    "def test_postgres_connection():\n",
    "    \"\"\"Test PostgreSQL connection using simple socket check.\"\"\"\n",
    "    import socket\n",
    "    \n",
    "    try:\n",
    "        # Test if PostgreSQL port is open\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(3)\n",
    "        result = sock.connect_ex(('localhost', 5432))\n",
    "        sock.close()\n",
    "        \n",
    "        if result == 0:\n",
    "            print(\"✓ PostgreSQL is accepting connections on port 5432!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ PostgreSQL port is not accessible\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not test PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "postgres_available = test_postgres_connection()\n",
    "\n",
    "if postgres_available:\n",
    "    print(\"\\n  Database Connection Details:\")\n",
    "    print(\"• Host: localhost\")\n",
    "    print(\"• Port: 5432\") \n",
    "    print(\"• Database: rag_db\")\n",
    "    print(\"• Username: rag_user\")\n",
    "    print(\"• Password: rag_password\")\n",
    "    \n",
    "    print(\"\\n  Recommended GUI Tools:\")\n",
    "    print(\"• DBeaver (Free): https://dbeaver.io/download/\")\n",
    "    print(\"• pgAdmin: https://www.pgadmin.org/download/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ok33sipeapa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PostgreSQL connected\n"
     ]
    }
   ],
   "source": [
    "# Test PostgreSQL Connection\n",
    "try:\n",
    "    import psycopg2\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"rag_db\", \n",
    "        user=\"rag_user\",\n",
    "        password=\"rag_password\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ PostgreSQL connected\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ psycopg2 not installed - basic connection only\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Database connection failed: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db723wrw0x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 total tables\n",
      "Application tables: 1\n",
      "Airflow tables: 0\n",
      "  • papers\n"
     ]
    }
   ],
   "source": [
    "# Check Database Tables\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'public'\n",
    "    ORDER BY table_name;\n",
    "\"\"\")\n",
    "\n",
    "all_tables = cursor.fetchall()\n",
    "\n",
    "app_tables = []\n",
    "airflow_tables = []\n",
    "\n",
    "for (table_name,) in all_tables:\n",
    "    if table_name in ['papers', 'users', 'embeddings']:\n",
    "        app_tables.append(table_name)\n",
    "    else:\n",
    "        airflow_tables.append(table_name)\n",
    "\n",
    "print(f\"Found {len(all_tables)} total tables\")\n",
    "print(f\"Application tables: {len(app_tables)}\")\n",
    "print(f\"Airflow tables: {len(airflow_tables)}\")\n",
    "\n",
    "for table in app_tables:\n",
    "    print(f\"  • {table}\")\n",
    "\n",
    "if not app_tables:\n",
    "    print(\"  No application tables yet (expected in Week 1)\")\n",
    "    \n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s58tqz40v58",
   "metadata": {},
   "source": [
    "## Project Commands\n",
    "\n",
    "**Makefile shortcuts:**\n",
    "```bash\n",
    "make start    # Start all services  \n",
    "make status   # Check service status\n",
    "make logs     # View logs\n",
    "make health   # Check service health\n",
    "make stop     # Stop all services\n",
    "make help     # View all commands\n",
    "```\n",
    "\n",
    "**Next:** Read the main `README.md` for complete project documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
